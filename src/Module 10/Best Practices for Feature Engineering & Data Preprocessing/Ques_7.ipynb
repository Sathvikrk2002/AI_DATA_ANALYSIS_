{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensuring Feature Consistency Between Training & InferencePipelines:\n",
    "\n",
    "**Task 1**: Consistent Feature Preparation\n",
    "- Step 1: Write a function for data preprocessing and imputation shared by both training and inference pipelines.\n",
    "- Step 2: Demonstrate consistent application on both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def preprocess_data(df, imputer=None, scaler=None, fit=True):\n",
    "    \"\"\"\n",
    "    Preprocess data consistently for training and inference.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): input data\n",
    "    - imputer (SimpleImputer): imputer instance (optional)\n",
    "    - scaler (StandardScaler): scaler instance (optional)\n",
    "    - fit (bool): if True, fit the imputer and scaler, else only transform\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: preprocessed data\n",
    "    - SimpleImputer: fitted imputer\n",
    "    - StandardScaler: fitted scaler\n",
    "    \"\"\"\n",
    "    # Select numeric columns for simplicity\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    data = df[numeric_cols]\n",
    "\n",
    "    # Initialize imputer and scaler if not provided\n",
    "    if imputer is None:\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "    # Impute missing values\n",
    "    if fit:\n",
    "        data_imputed = imputer.fit_transform(data)\n",
    "    else:\n",
    "        data_imputed = imputer.transform(data)\n",
    "\n",
    "    # Scale features\n",
    "    if fit:\n",
    "        data_scaled = scaler.fit_transform(data_imputed)\n",
    "    else:\n",
    "        data_scaled = scaler.transform(data_imputed)\n",
    "\n",
    "    # Return as DataFrame with original column names\n",
    "    processed_df = pd.DataFrame(data_scaled, columns=numeric_cols, index=df.index)\n",
    "\n",
    "    return processed_df, imputer, scaler\n",
    "\n",
    "# Example Usage:\n",
    "\n",
    "# Sample training data with missing values\n",
    "train_df = pd.DataFrame({\n",
    "    'age': [25, 30, np.nan, 22, 40],\n",
    "    'income': [50000, 60000, 55000, np.nan, 65000],\n",
    "    'score': [200, 220, 210, 215, np.nan]\n",
    "})\n",
    "\n",
    "# Sample inference data (new data)\n",
    "inference_df = pd.DataFrame({\n",
    "    'age': [28, np.nan, 35],\n",
    "    'income': [52000, 58000, np.nan],\n",
    "    'score': [205, 210, 215]\n",
    "})\n",
    "\n",
    "# Preprocess training data (fit)\n",
    "train_processed, imputer, scaler = preprocess_data(train_df, fit=True)\n",
    "print(\"Processed Training Data:\")\n",
    "print(train_processed)\n",
    "\n",
    "# Preprocess inference data (transform only)\n",
    "inference_processed, _, _ = preprocess_data(inference_df, imputer=imputer, scaler=scaler, fit=False)\n",
    "print(\"\\nProcessed Inference Data:\")\n",
    "print(inference_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Pipeline Integration\n",
    "- Step 1: Use sklearn pipelines to encapsulate the preprocessing steps.\n",
    "- Step 2: Configure identical pipelines for both training and building inference models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Sample training data\n",
    "train_df = pd.DataFrame({\n",
    "    'age': [25, 30, np.nan, 22, 40],\n",
    "    'income': [50000, 60000, 55000, np.nan, 65000],\n",
    "    'score': [200, 220, 210, 215, np.nan],\n",
    "    'target': [0, 1, 0, 1, 0]\n",
    "})\n",
    "\n",
    "# Sample inference data (no target here)\n",
    "inference_df = pd.DataFrame({\n",
    "    'age': [28, np.nan, 35],\n",
    "    'income': [52000, 58000, np.nan],\n",
    "    'score': [205, 210, 215]\n",
    "})\n",
    "\n",
    "# Define feature columns and target\n",
    "feature_cols = ['age', 'income', 'score']\n",
    "target_col = 'target'\n",
    "\n",
    "# Build pipeline for preprocessing and classification\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Training phase\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df[target_col]\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# After training, use the same pipeline for inference (transform + predict)\n",
    "X_infer = inference_df[feature_cols]\n",
    "predictions = pipeline.predict(X_infer)\n",
    "\n",
    "print(\"Predictions on inference data:\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Saving and Loading Preprocessing Models\n",
    "- Step 1: Save the transformation model after fitting it to the training data.\n",
    "- Step 2: Load and apply the saved model during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib  # for saving and loading models\n",
    "\n",
    "# Sample training data\n",
    "train_df = pd.DataFrame({\n",
    "    'age': [25, 30, np.nan, 22, 40],\n",
    "    'income': [50000, 60000, 55000, np.nan, 65000],\n",
    "    'score': [200, 220, 210, 215, np.nan],\n",
    "    'target': [0, 1, 0, 1, 0]\n",
    "})\n",
    "\n",
    "feature_cols = ['age', 'income', 'score']\n",
    "target_col = 'target'\n",
    "\n",
    "# Build pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Training phase\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df[target_col]\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Save the pipeline to a file\n",
    "joblib.dump(pipeline, 'trained_pipeline.joblib')\n",
    "\n",
    "# ----- Later or in inference script -----\n",
    "\n",
    "# Load the saved pipeline\n",
    "loaded_pipeline = joblib.load('trained_pipeline.joblib')\n",
    "\n",
    "# Sample inference data (no target)\n",
    "inference_df = pd.DataFrame({\n",
    "    'age': [28, np.nan, 35],\n",
    "    'income': [52000, 58000, np.nan],\n",
    "    'score': [205, 210, 215]\n",
    "})\n",
    "X_infer = inference_df[feature_cols]\n",
    "\n",
    "# Use loaded pipeline for prediction (imputation + scaling + classification)\n",
    "predictions = loaded_pipeline.predict(X_infer)\n",
    "\n",
    "print(\"Predictions using loaded pipeline:\")\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
