{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Understanding and Defining Data Quality Metrics\n",
    "**Description**: Learn how to define basic data quality metrics such as completeness, validity, and uniqueness for a simple dataset.\n",
    "\n",
    "**Steps**:\n",
    "1. Dataset: Use a CSV with columns like Name , Email , Age .\n",
    "2. Metric Definitions:\n",
    "    - Completeness: Percentage of non-null values.\n",
    "    - Validity: % of email fields containing @ .\n",
    "    - Uniqueness: Count distinct entries in the Email column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completeness (% of non-null values):\n",
      "Name      66.666667\n",
      "Email    100.000000\n",
      "Age       83.333333\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "Validity (% of emails containing '@'): 100.00%\n",
      "\n",
      "\n",
      "Uniqueness (distinct emails): 5 out of 6\n",
      "Uniqueness (% of distinct emails): 83.33%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Create a sample CSV dataset\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', '', 'David', 'Eve', None],\n",
    "    'Email': ['alice@example.com', 'bob@example', 'charlie@domain.com', 'david@example.com', 'eve@example.com', 'eve@example.com'],\n",
    "    'Age': [25, None, 30, 28, 22, 35]\n",
    "}\n",
    "\n",
    "# Save the sample dataset to a CSV file\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('sample_data.csv', index=False)\n",
    "\n",
    "# Step 2: Load the CSV file\n",
    "df = pd.read_csv('sample_data.csv')\n",
    "\n",
    "# Step 3: Calculate Data Quality Metrics\n",
    "\n",
    "# Completeness: Percentage of non-null values for each column\n",
    "completeness = df.notnull().mean() * 100\n",
    "print(\"Completeness (% of non-null values):\")\n",
    "print(completeness)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Validity: Percentage of email fields containing '@'\n",
    "valid_emails = df['Email'].str.contains('@', na=False).mean() * 100\n",
    "print(f\"Validity (% of emails containing '@'): {valid_emails:.2f}%\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Uniqueness: Count distinct entries in the Email column\n",
    "unique_emails = df['Email'].nunique()\n",
    "total_emails = df['Email'].count()\n",
    "uniqueness_percentage = (unique_emails / total_emails) * 100\n",
    "print(f\"Uniqueness (distinct emails): {unique_emails} out of {total_emails}\")\n",
    "print(f\"Uniqueness (% of distinct emails): {uniqueness_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Calculating Data Quality Score\n",
    "**Description**: Aggregate multiple metrics to calculate an overall data quality score.\n",
    "\n",
    "**Steps**:\n",
    "1. Formula: Simple average of all metrics defined in Task 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Metrics:\n",
      "Completeness (avg % non-null): 83.33%\n",
      "Validity (% emails with '@'): 100.00%\n",
      "Uniqueness (% distinct emails): 83.33%\n",
      "\n",
      "\n",
      "Overall Data Quality Score: 88.89%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load the CSV file (assumes sample_data.csv from Task 1)\n",
    "df = pd.read_csv('sample_data.csv')\n",
    "\n",
    "# Step 2: Calculate Data Quality Metrics (from Task 1)\n",
    "\n",
    "# Completeness: Average percentage of non-null values across all columns\n",
    "completeness = df.notnull().mean().mean() * 100\n",
    "\n",
    "# Validity: Percentage of email fields containing '@'\n",
    "valid_emails = df['Email'].str.contains('@', na=False).mean() * 100\n",
    "\n",
    "# Uniqueness: Percentage of distinct entries in the Email column\n",
    "unique_emails = df['Email'].nunique()\n",
    "total_emails = df['Email'].count()\n",
    "uniqueness_percentage = (unique_emails / total_emails) * 100\n",
    "\n",
    "# Step 3: Calculate Overall Data Quality Score\n",
    "# Simple average of completeness, validity, and uniqueness\n",
    "data_quality_score = (completeness + valid_emails + uniqueness_percentage) / 3\n",
    "\n",
    "# Step 4: Display Results\n",
    "print(\"Individual Metrics:\")\n",
    "print(f\"Completeness (avg % non-null): {completeness:.2f}%\")\n",
    "print(f\"Validity (% emails with '@'): {valid_emails:.2f}%\")\n",
    "print(f\"Uniqueness (% distinct emails): {uniqueness_percentage:.2f}%\")\n",
    "print(\"\\n\")\n",
    "print(f\"Overall Data Quality Score: {data_quality_score:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Creating Expectations for a CSV\n",
    "**Description**: Develop basic data quality expectations using Great Expectations.\n",
    "\n",
    "**Steps**:\n",
    "1. Expectation Suite\n",
    "2. Define Expectations for Completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error initializing DataContext: type object 'FileDataContext' has no attribute 'create'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'FileDataContext' has no attribute 'create'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Step 2: Initialize a FileDataContext\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 19\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43mFileDataContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m(project_root_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitialized Great Expectations DataContext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'FileDataContext' has no attribute 'create'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import great_expectations as ge\n",
    "from great_expectations.data_context import FileDataContext\n",
    "import os\n",
    "\n",
    "# Step 1: Create sample_data.csv if it doesn't exist\n",
    "if not os.path.exists('sample_data.csv'):\n",
    "    data = {\n",
    "        'Name': ['Alice', 'Bob', '', 'David', 'Eve', None],\n",
    "        'Email': ['alice@example.com', 'bob@example', 'charlie@domain.com', 'david@example.com', 'eve@example.com', 'eve@example.com'],\n",
    "        'Age': [25, None, 30, 28, 22, 35]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv('sample_data.csv', index=False)\n",
    "    print(\"Created sample_data.csv\")\n",
    "\n",
    "# Step 2: Initialize a FileDataContext\n",
    "try:\n",
    "    context = FileDataContext.create(project_root_dir=\".\")\n",
    "    print(\"Initialized Great Expectations DataContext\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing DataContext: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 3: Set up Datasource and Data Asset\n",
    "datasource_name = \"pandas_datasource\"\n",
    "data_asset_name = \"sample_data\"\n",
    "\n",
    "try:\n",
    "    if datasource_name not in context.datasources:\n",
    "        datasource = context.sources.add_pandas(name=datasource_name)\n",
    "    else:\n",
    "        datasource = context.datasources[datasource_name]\n",
    "    \n",
    "    data_asset = datasource.add_csv_asset(name=data_asset_name, filepath_or_buffer=\"sample_data.csv\")\n",
    "    print(f\"Added datasource '{datasource_name}' and asset '{data_asset_name}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error setting up datasource/asset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 4: Create or load Expectation Suite\n",
    "suite_name = \"sample_data_suite\"\n",
    "try:\n",
    "    if suite_name not in context.list_expectation_suite_names():\n",
    "        suite = context.add_expectation_suite(expectation_suite_name=suite_name)\n",
    "    else:\n",
    "        suite = context.get_expectation_suite(expectation_suite_name=suite_name)\n",
    "    print(f\"Expectation suite '{suite_name}' ready\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating/loading expectation suite: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 5: Get a Validator\n",
    "try:\n",
    "    batch_request = data_asset.build_batch_request()\n",
    "    validator = context.get_validator(batch_request=batch_request, expectation_suite_name=suite_name)\n",
    "    print(\"Validator initialized\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing validator: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 6: Define Completeness Expectations\n",
    "try:\n",
    "    validator.expect_column_values_to_not_be_null(column=\"Email\", mostly=0.95, result_format=\"SUMMARY\")\n",
    "    validator.expect_column_values_to_not_be_null(column=\"Name\", mostly=0.80, result_format=\"SUMMARY\")\n",
    "    validator.expect_column_values_to_not_be_null(column=\"Age\", mostly=0.80, result_format=\"SUMMARY\")\n",
    "    print(\"Completeness expectations defined\")\n",
    "except Exception as e:\n",
    "    print(f\"Error defining expectations: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 7: Save the Expectation Suite\n",
    "try:\n",
    "    validator.save_expectation_suite(discard_failed_expectations=False)\n",
    "    print(f\"Expectation suite '{suite_name}' saved to Great Expectations configuration\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving expectation suite: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 8: Validate the Dataset\n",
    "try:\n",
    "    checkpoint = context.add_or_update_checkpoint(\n",
    "        name=\"sample_checkpoint\",\n",
    "        validations=[\n",
    "            {\n",
    "                \"batch_request\": batch_request,\n",
    "                \"expectation_suite_name\": suite_name,\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    checkpoint_result = checkpoint.run()\n",
    "    print(\"\\nValidation Results:\")\n",
    "    print(checkpoint_result)\n",
    "except Exception as e:\n",
    "    print(f\"Error running validation: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 9: Save the Expectation Suite to a File\n",
    "try:\n",
    "    context.save_expectation_suite(expectation_suite=validator.expectation_suite, expectation_suite_name=suite_name)\n",
    "    print(f\"\\nExpectation suite saved to Great Expectations configuration\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving expectation suite to file: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Running and Validating Expectations\n",
    "**Description**: Run the created expectations and generate an output report.\n",
    "\n",
    "**Steps**:\n",
    "1. Validate\n",
    "2. Generate HTML Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Great Expectations DataContext\n",
      "Error setting up datasource/asset: 'FileDataContext' object has no attribute 'datasources'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FileDataContext' object has no attribute 'datasources'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m data_asset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m datasource_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasources\u001b[49m:\n\u001b[1;32m     31\u001b[0m         datasource \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39msources\u001b[38;5;241m.\u001b[39madd_pandas(name\u001b[38;5;241m=\u001b[39mdatasource_name)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FileDataContext' object has no attribute 'datasources'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import great_expectations as ge\n",
    "from great_expectations.data_context import FileDataContext\n",
    "import os\n",
    "\n",
    "# Step 1: Verify sample_data.csv exists, create if not\n",
    "if not os.path.exists('sample_data.csv'):\n",
    "    data = {\n",
    "        'Name': ['Alice', 'Bob', '', 'David', 'Eve', None],\n",
    "        'Email': ['alice@example.com', 'bob@example', 'charlie@domain.com', 'david@example.com', 'eve@example.com', 'eve@example.com'],\n",
    "        'Age': [25, None, 30, 28, 22, 35]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv('sample_data.csv', index=False)\n",
    "    print(\"Created sample_data.csv\")\n",
    "\n",
    "# Step 2: Initialize FileDataContext\n",
    "try:\n",
    "    context = FileDataContext(project_root_dir=\".\")\n",
    "    print(\"Initialized Great Expectations DataContext\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing DataContext: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 3: Verify Datasource and Data Asset\n",
    "datasource_name = \"pandas_datasource\"\n",
    "data_asset_name = \"sample_data\"\n",
    "\n",
    "try:\n",
    "    if datasource_name not in context.datasources:\n",
    "        datasource = context.sources.add_pandas(name=datasource_name)\n",
    "    else:\n",
    "        datasource = context.datasources[datasource_name]\n",
    "    \n",
    "    if data_asset_name not in [asset.name for asset in datasource.get_asset_names()]:\n",
    "        data_asset = datasource.add_csv_asset(name=data_asset_name, filepath_or_buffer=\"sample_data.csv\")\n",
    "    else:\n",
    "        data_asset = datasource.get_asset(data_asset_name)\n",
    "    print(f\"Verified datasource '{datasource_name}' and asset '{data_asset_name}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error setting up datasource/asset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 4: Load Expectation Suite\n",
    "suite_name = \"sample_data_suite\"\n",
    "try:\n",
    "    suite = context.get_expectation_suite(expectation_suite_name=suite_name)\n",
    "    print(f\"Loaded expectation suite '{suite_name}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading expectation suite: {e}\")\n",
    "    print(\"Please ensure Task 3 was completed to create the expectation suite.\")\n",
    "    raise\n",
    "\n",
    "# Step 5: Get a Validator and Validate\n",
    "try:\n",
    "    batch_request = data_asset.build_batch_request()\n",
    "    validator = context.get_validator(batch_request=batch_request, expectation_suite_name=suite_name)\n",
    "    print(\"Validator initialized\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing validator: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 6: Run Validation\n",
    "try:\n",
    "    checkpoint = context.add_or_update_checkpoint(\n",
    "        name=\"sample_checkpoint\",\n",
    "        validations=[\n",
    "            {\n",
    "                \"batch_request\": batch_request,\n",
    "                \"expectation_suite_name\": suite_name,\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    checkpoint_result = checkpoint.run()\n",
    "    print(\"\\nValidation completed\")\n",
    "except Exception as e:\n",
    "    print(f\"Error running validation: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 7: Generate HTML Report\n",
    "try:\n",
    "    # Build and save Data Docs (HTML report)\n",
    "    context.build_data_docs()\n",
    "    print(\"\\nHTML report generated in the Great Expectations Data Docs directory\")\n",
    "    \n",
    "    # Get the URL of the validation results\n",
    "    validation_results_url = context.get_validation_result_page_url(checkpoint_result)\n",
    "    print(f\"Validation results available at: {validation_results_url}\")\n",
    "    \n",
    "    # Save the HTML report to a specific file\n",
    "    report_path = \"validation_report.html\"\n",
    "    with open(report_path, \"w\") as f:\n",
    "        f.write(context.get_data_docs_page(checkpoint_result))\n",
    "    print(f\"HTML report saved to '{report_path}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error generating HTML report: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Automating Data Quality Score Calculation\n",
    "**Description**: Automate the data quality score via a script that integrates with Great\n",
    "Expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded sample_data.csv\n",
      "Initialized Great Expectations DataContext\n",
      "Error setting up datasource/asset: 'FileDataContext' object has no attribute 'datasources'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FileDataContext' object has no attribute 'datasources'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m data_asset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m datasource_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasources\u001b[49m:\n\u001b[1;32m     39\u001b[0m         datasource \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39msources\u001b[38;5;241m.\u001b[39madd_pandas(name\u001b[38;5;241m=\u001b[39mdatasource_name)\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FileDataContext' object has no attribute 'datasources'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import great_expectations as ge\n",
    "from great_expectations.data_context import FileDataContext\n",
    "import os\n",
    "\n",
    "# Step 1: Verify sample_data.csv exists, create if not\n",
    "if not os.path.exists('sample_data.csv'):\n",
    "    data = {\n",
    "        'Name': ['Alice', 'Bob', '', 'David', 'Eve', None],\n",
    "        'Email': ['alice@example.com', 'bob@example', 'charlie@domain.com', 'david@example.com', 'eve@example.com', 'eve@example.com'],\n",
    "        'Age': [25, None, 30, 28, 22, 35]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv('sample_data.csv', index=False)\n",
    "    print(\"Created sample_data.csv\")\n",
    "\n",
    "# Step 2: Load the CSV file\n",
    "try:\n",
    "    df = pd.read_csv('sample_data.csv')\n",
    "    print(\"Loaded sample_data.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 3: Initialize FileDataContext\n",
    "try:\n",
    "    context = FileDataContext(project_root_dir=\".\")\n",
    "    print(\"Initialized Great Expectations DataContext\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing DataContext: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 4: Verify Datasource and Data Asset\n",
    "datasource_name = \"pandas_datasource\"\n",
    "data_asset_name = \"sample_data\"\n",
    "\n",
    "try:\n",
    "    if datasource_name not in context.datasources:\n",
    "        datasource = context.sources.add_pandas(name=datasource_name)\n",
    "    else:\n",
    "        datasource = context.datasources[datasource_name]\n",
    "    \n",
    "    if data_asset_name not in [asset.name for asset in datasource.get_asset_names()]:\n",
    "        data_asset = datasource.add_csv_asset(name=data_asset_name, filepath_or_buffer=\"sample_data.csv\")\n",
    "    else:\n",
    "        data_asset = datasource.get_asset(data_asset_name)\n",
    "    print(f\"Verified datasource '{datasource_name}' and asset '{data_asset_name}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error setting up datasource/asset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 5: Load Expectation Suite\n",
    "suite_name = \"sample_data_suite\"\n",
    "try:\n",
    "    suite = context.get_expectation_suite(expectation_suite_name=suite_name)\n",
    "    print(f\"Loaded expectation suite '{suite_name}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading expectation suite: {e}\")\n",
    "    print(\"Please ensure Task 3 was completed to create the expectation suite.\")\n",
    "    raise\n",
    "\n",
    "# Step 6: Run Validation with Great Expectations\n",
    "try:\n",
    "    batch_request = data_asset.build_batch_request()\n",
    "    validator = context.get_validator(batch_request=batch_request, expectation_suite_name=suite_name)\n",
    "    checkpoint = context.add_or_update_checkpoint(\n",
    "        name=\"sample_checkpoint\",\n",
    "        validations=[\n",
    "            {\n",
    "                \"batch_request\": batch_request,\n",
    "                \"expectation_suite_name\": suite_name,\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    checkpoint_result = checkpoint.run()\n",
    "    print(\"\\nGreat Expectations validation completed\")\n",
    "except Exception as e:\n",
    "    print(f\"Error running validation: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 7: Calculate Data Quality Metrics\n",
    "try:\n",
    "    # Completeness: Average percentage of non-null values across all columns\n",
    "    completeness = df.notnull().mean().mean() * 100\n",
    "\n",
    "    # Validity: Percentage of email fields containing '@'\n",
    "    valid_emails = df['Email'].str.contains('@', na=False).mean() * 100\n",
    "\n",
    "    # Uniqueness: Percentage of distinct entries in the Email column\n",
    "    unique_emails = df['Email'].nunique()\n",
    "    total_emails = df['Email'].count()\n",
    "    uniqueness_percentage = (unique_emails / total_emails) * 100\n",
    "\n",
    "    # Overall Data Quality Score: Simple average of metrics\n",
    "    data_quality_score = (completeness + valid_emails + uniqueness_percentage) / 3\n",
    "\n",
    "    print(\"\\nIndividual Metrics:\")\n",
    "    print(f\"Completeness (avg % non-null): {completeness:.2f}%\")\n",
    "    print(f\"Validity (% emails with '@'): {valid_emails:.2f}%\")\n",
    "    print(f\"Uniqueness (% distinct emails): {uniqueness_percentage:.2f}%\")\n",
    "    print(f\"\\nOverall Data Quality Score: {data_quality_score:.2f}%\")\n",
    "except Exception as e:\n",
    "    print(f\"Error calculating metrics: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 8: Generate HTML Report\n",
    "try:\n",
    "    # Add data quality score to the context for inclusion in Data Docs\n",
    "    context.variables.metadata = {\n",
    "        \"data_quality_score\": f\"{data_quality_score:.2f}%\",\n",
    "        \"completeness\": f\"{completeness:.2f}%\",\n",
    "        \"validity\": f\"{valid_emails:.2f}%\",\n",
    "        \"uniqueness\": f\"{uniqueness_percentage:.2f}%\"\n",
    "    }\n",
    "    context.save_context()\n",
    "\n",
    "    # Build and save Data Docs (HTML report)\n",
    "    context.build_data_docs()\n",
    "    print(\"\\nHTML report generated in the Great Expectations Data Docs directory\")\n",
    "    \n",
    "    # Save a standalone HTML report\n",
    "    report_path = \"data_quality_report.html\"\n",
    "    with open(report_path, \"w\") as f:\n",
    "        f.write(context.get_data_docs_page(checkpoint_result))\n",
    "    print(f\"HTML report saved to '{report_path}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error generating HTML report: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Leveraging Data Quality Metrics for Automated Data Cleaning\n",
    "**Description**: Implement a system where if data quality metrics fall below a threshold,\n",
    "automated data cleaning scripts are triggered.\n",
    "\n",
    "**Steps**:\n",
    "1. Define Cleaning Logic\n",
    "2. Integrate with Great Expectations:\n",
    "    - Use an action within the Great Expectations action list that only triggers if quality score is below a threshold, automating the cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded sample_data.csv\n",
      "Initialized Great Expectations DataContext\n",
      "Error setting up datasource/asset: 'FileDataContext' object has no attribute 'datasources'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FileDataContext' object has no attribute 'datasources'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 75\u001b[0m\n\u001b[1;32m     72\u001b[0m data_asset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m datasource_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasources\u001b[49m:\n\u001b[1;32m     76\u001b[0m         datasource \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39msources\u001b[38;5;241m.\u001b[39madd_pandas(name\u001b[38;5;241m=\u001b[39mdatasource_name)\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FileDataContext' object has no attribute 'datasources'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import great_expectations as ge\n",
    "from great_expectations.data_context import FileDataContext\n",
    "from great_expectations.core.expectation_suite import ExpectationSuite\n",
    "from great_expectations.checkpoint.actions import ValidationAction\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Custom Great Expectations Action to Trigger Cleaning\n",
    "class CustomCleaningAction(ValidationAction):\n",
    "    def run(self, validation_result, context, expectation_suite_name, batch_request):\n",
    "        # Extract data quality score from context metadata\n",
    "        data_quality_score = float(context.variables.metadata.get(\"data_quality_score\", 0))\n",
    "        threshold = 90.0  # Threshold for triggering cleaning\n",
    "        \n",
    "        if data_quality_score < threshold:\n",
    "            print(f\"Data quality score ({data_quality_score:.2f}%) is below threshold ({threshold}%). Triggering cleaning...\")\n",
    "            \n",
    "            # Load the dataset\n",
    "            df = pd.read_csv('sample_data.csv')\n",
    "            \n",
    "            # Cleaning Logic\n",
    "            # Completeness: Fill missing Name with 'Unknown', Age with median\n",
    "            df['Name'] = df['Name'].fillna('Unknown')\n",
    "            df['Age'] = df['Age'].fillna(df['Age'].median())\n",
    "            \n",
    "            # Validity: Remove rows where Email does not contain '@'\n",
    "            df = df[df['Email'].str.contains('@', na=False)]\n",
    "            \n",
    "            # Uniqueness: Drop duplicate Emails, keeping first occurrence\n",
    "            df = df.drop_duplicates(subset='Email', keep='first')\n",
    "            \n",
    "            # Save cleaned dataset\n",
    "            cleaned_path = 'sample_data_cleaned.csv'\n",
    "            df.to_csv(cleaned_path, index=False)\n",
    "            print(f\"Cleaned dataset saved to '{cleaned_path}'\")\n",
    "            \n",
    "            return {\"cleaned\": True, \"cleaned_file\": cleaned_path}\n",
    "        else:\n",
    "            print(f\"Data quality score ({data_quality_score:.2f}%) meets threshold ({threshold}%). No cleaning needed.\")\n",
    "            return {\"cleaned\": False}\n",
    "\n",
    "# Step 1: Verify sample_data.csv exists, create if not\n",
    "if not os.path.exists('sample_data.csv'):\n",
    "    data = {\n",
    "        'Name': ['Alice', 'Bob', '', 'David', 'Eve', None],\n",
    "        'Email': ['alice@example.com', 'bob@example', 'charlie@domain.com', 'david@example.com', 'eve@example.com', 'eve@example.com'],\n",
    "        'Age': [25, None, 30, 28, 22, 35]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv('sample_data.csv', index=False)\n",
    "    print(\"Created sample_data.csv\")\n",
    "\n",
    "# Step 2: Load the CSV file\n",
    "try:\n",
    "    df = pd.read_csv('sample_data.csv')\n",
    "    print(\"Loaded sample_data.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 3: Initialize FileDataContext\n",
    "try:\n",
    "    context = FileDataContext(project_root_dir=\".\")\n",
    "    print(\"Initialized Great Expectations DataContext\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing DataContext: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 4: Verify Datasource and Data Asset\n",
    "datasource_name = \"pandas_datasource\"\n",
    "data_asset_name = \"sample_data\"\n",
    "\n",
    "try:\n",
    "    if datasource_name not in context.datasources:\n",
    "        datasource = context.sources.add_pandas(name=datasource_name)\n",
    "    else:\n",
    "        datasource = context.datasources[datasource_name]\n",
    "    \n",
    "    if data_asset_name not in [asset.name for asset in datasource.get_asset_names()]:\n",
    "        data_asset = datasource.add_csv_asset(name=data_asset_name, filepath_or_buffer=\"sample_data.csv\")\n",
    "    else:\n",
    "        data_asset = datasource.get_asset(data_asset_name)\n",
    "    print(f\"Verified datasource '{datasource_name}' and asset '{data_asset_name}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error setting up datasource/asset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 5: Load Expectation Suite\n",
    "suite_name = \"sample_data_suite\"\n",
    "try:\n",
    "    suite = context.get_expectation_suite(expectation_suite_name=suite_name)\n",
    "    print(f\"Loaded expectation suite '{suite_name}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading expectation suite: {e}\")\n",
    "    print(\"Please ensure Task 3 was completed to create the expectation suite.\")\n",
    "    raise\n",
    "\n",
    "# Step 6: Calculate Data Quality Metrics\n",
    "try:\n",
    "    # Completeness: Average percentage of non-null values across all columns\n",
    "    completeness = df.notnull().mean().mean() * 100\n",
    "\n",
    "    # Validity: Percentage of email fields containing '@'\n",
    "    valid_emails = df['Email'].str.contains('@', na=False).mean() * 100\n",
    "\n",
    "    # Uniqueness: Percentage of distinct entries in the Email column\n",
    "    unique_emails = df['Email'].nunique()\n",
    "    total_emails = df['Email'].count()\n",
    "    uniqueness_percentage = (unique_emails / total_emails) * 100\n",
    "\n",
    "    # Overall Data Quality Score: Simple average of metrics\n",
    "    data_quality_score = (completeness + valid_emails + uniqueness_percentage) / 3\n",
    "\n",
    "    # Store metrics in context metadata\n",
    "    context.variables.metadata = {\n",
    "        \"data_quality_score\": f\"{data_quality_score:.2f}\",\n",
    "        \"completeness\": f\"{completeness:.2f}%\",\n",
    "        \"validity\": f\"{valid_emails:.2f}%\",\n",
    "        \"uniqueness\": f\"{uniqueness_percentage:.2f}%\"\n",
    "    }\n",
    "    context.save_context()\n",
    "\n",
    "    print(\"\\nIndividual Metrics:\")\n",
    "    print(f\"Completeness (avg % non-null): {completeness:.2f}%\")\n",
    "    print(f\"Validity (% emails with '@'): {valid_emails:.2f}%\")\n",
    "    print(f\"Uniqueness (% distinct emails): {uniqueness_percentage:.2f}%\")\n",
    "    print(f\"\\nOverall Data Quality Score: {data_quality_score:.2f}%\")\n",
    "except Exception as e:\n",
    "    print(f\"Error calculating metrics: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 7: Run Validation with Custom Action\n",
    "try:\n",
    "    batch_request = data_asset.build_batch_request()\n",
    "    checkpoint = context.add_or_update_checkpoint(\n",
    "        name=\"sample_checkpoint\",\n",
    "        validations=[\n",
    "            {\n",
    "                \"batch_request\": batch_request,\n",
    "                \"expectation_suite_name\": suite_name,\n",
    "                \"action_list\": [\n",
    "                    {\n",
    "                        \"name\": \"custom_cleaning_action\",\n",
    "                        \"action\": {\n",
    "                            \"class_name\": \"CustomCleaningAction\",\n",
    "                            \"module_name\": __name__\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    checkpoint_result = checkpoint.run()\n",
    "    print(\"\\nValidation and custom action completed\")\n",
    "except Exception as e:\n",
    "    print(f\"Error running validation/action: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 8: Generate HTML Report\n",
    "try:\n",
    "    context.build_data_docs()\n",
    "    print(\"\\nHTML report generated in the Great Expectations Data Docs directory\")\n",
    "    \n",
    "    report_path = \"data_quality_cleaning_report.html\"\n",
    "    with open(report_path, \"w\") as f:\n",
    "        f.write(context.get_data_docs_page(checkpoint_result))\n",
    "    print(f\"HTML report saved to '{report_path}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error generating HTML report: {e}\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
