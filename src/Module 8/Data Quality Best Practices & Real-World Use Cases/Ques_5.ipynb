{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI/ML â€“ Improving Model Performance with Clean Data\n",
    "\n",
    "**Task 1**: Data Preprocessing for Models\n",
    "\n",
    "**Objective**: Enhance data quality for better AI/ML outcomes.\n",
    "\n",
    "**Steps**:\n",
    "1. Choose a dataset for training an AI/ML model.\n",
    "2. Identify common data issues like null values, redundant features, or noisydata.\n",
    "3. Apply preprocessing methods such as imputation, normalization, or feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data with Issues:\n",
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                NaN               3.5                1.4               0.2   \n",
      "1                NaN               3.0                1.4               0.2   \n",
      "2                NaN               3.2                1.3               0.2   \n",
      "3                NaN               3.1                1.5               0.2   \n",
      "4                NaN               3.6                1.4               0.2   \n",
      "\n",
      "   redundant_feature  noisy_feature  \n",
      "0                  1       9.986995  \n",
      "1                  1       4.136855  \n",
      "2                  1       7.782418  \n",
      "3                  1      -2.107377  \n",
      "4                  1      -1.119679  \n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Step 1: Load dataset\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = iris.target\n",
    "\n",
    "# Introduce data quality issues\n",
    "X.loc[0:5, 'sepal length (cm)'] = np.nan  # Inject missing values\n",
    "X['redundant_feature'] = 1  # Add a constant column (zero variance)\n",
    "X['noisy_feature'] = np.random.normal(0, 10, size=len(X))  # Add noise\n",
    "\n",
    "print(\"Original Data with Issues:\")\n",
    "print(X.head())\n",
    "\n",
    "# Step 2: Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# Step 3: Remove low variance (redundant) features\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "X_selected = pd.DataFrame(selector.fit_transform(X_imputed),\n",
    "                          columns=X_imputed.columns[selector.get_support()])\n",
    "\n",
    "# Step 4: Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X_selected),\n",
    "                        columns=X_selected.columns)\n",
    "\n",
    "# Step 5: Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 6: Train model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Evaluate Model Performance\n",
    "\n",
    "**Objective**: Assess the impact of data quality improvements on model performance.\n",
    "\n",
    "**Steps**:\n",
    "1. Train a simple ML model with and without preprocessing.\n",
    "2. Analyze and compare model performance metrics to evaluate the impact of data quality strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš« Model WITHOUT Preprocessing:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n",
      "\n",
      "âœ… Model WITH Preprocessing:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = iris.target\n",
    "\n",
    "# Introduce issues (simulate low data quality)\n",
    "X_dirty = X.copy()\n",
    "X_dirty.loc[0:5, 'sepal length (cm)'] = np.nan           # Add missing values\n",
    "X_dirty['redundant_feature'] = 1                        # Add constant feature\n",
    "X_dirty['noisy_feature'] = np.random.normal(0, 10, X.shape[0])  # Add noisy feature\n",
    "\n",
    "# ----------- Model 1: WITHOUT Preprocessing -----------\n",
    "# Fill missing values with 0 to allow training, skip real preprocessing\n",
    "X_dirty_filled = X_dirty.fillna(0)\n",
    "\n",
    "# Train/test split\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X_dirty_filled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model1 = RandomForestClassifier(random_state=42)\n",
    "model1.fit(X_train1, y_train1)\n",
    "\n",
    "# Predict and report\n",
    "y_pred1 = model1.predict(X_test1)\n",
    "print(\"ðŸš« Model WITHOUT Preprocessing:\\n\")\n",
    "print(classification_report(y_test1, y_pred1))\n",
    "\n",
    "\n",
    "# ----------- Model 2: WITH Preprocessing -----------\n",
    "# Imputation\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = pd.DataFrame(imputer.fit_transform(X_dirty), columns=X_dirty.columns)\n",
    "\n",
    "# Remove low variance (redundant) features\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "X_selected = pd.DataFrame(selector.fit_transform(X_imputed),\n",
    "                          columns=X_imputed.columns[selector.get_support()])\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X_selected), columns=X_selected.columns)\n",
    "\n",
    "# Train/test split\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model2 = RandomForestClassifier(random_state=42)\n",
    "model2.fit(X_train2, y_train2)\n",
    "\n",
    "\n",
    "# Predict and report\n",
    "y_pred2 = model2.predict(X_test2)\n",
    "print(\"\\nâœ… Model WITH Preprocessing:\\n\")\n",
    "print(classification_report(y_test2, y_pred2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
