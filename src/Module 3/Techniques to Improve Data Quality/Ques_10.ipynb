{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplication using Agglomerative Clustering (TF-IDF):\n",
      "            name                      email          address  cluster_tfidf\n",
      "0    Alice Smith    alice.smith@example.com      123 Main St              0\n",
      "1    Bob Johnson    bob.johnson@example.com      456 Oak Ave              2\n",
      "2   Alise Smithe   alise.smithe@example.com  123 Main Street              0\n",
      "3     Bob Jonson     bob.jonson@example.com   456 Oak Avenue              2\n",
      "4  Charlie Brown  charlie.brown@example.com      789 Pine Ln              1\n",
      "5  Charlie Brown  charlie.brown@example.com    789 Pine Lane              1\n",
      "\n",
      "Classification Report for Duplicate Detection:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           1.00         3\n",
      "   macro avg       1.00      1.00      1.00         3\n",
      "weighted avg       1.00      1.00      1.00         3\n",
      "\n",
      "\n",
      "Identified Duplicate Groups (based on classification):\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# 1. Sample Data (Replace with your actual data loading)\n",
    "data = pd.DataFrame({\n",
    "    'id': [1, 2, 3, 4, 5, 6],\n",
    "    'name': ['Alice Smith', 'Bob Johnson', 'Alise Smithe', 'Bob Jonson', 'Charlie Brown', 'Charlie Brown'],\n",
    "    'email': ['alice.smith@example.com', 'bob.johnson@example.com', 'alise.smithe@example.com', 'bob.jonson@example.com', 'charlie.brown@example.com', 'charlie.brown@example.com'],\n",
    "    'address': ['123 Main St', '456 Oak Ave', '123 Main Street', '456 Oak Avenue', '789 Pine Ln', '789 Pine Lane']\n",
    "})\n",
    "\n",
    "# 2. Feature Engineering\n",
    "# Combine relevant columns into a single text feature for simplicity\n",
    "data['combined_features'] = data['name'] + ' ' + data['email'] + ' ' + data['address']\n",
    "\n",
    "# Option 1: Using TF-IDF for text similarity\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data['combined_features'])\n",
    "\n",
    "# 3. Similarity Calculation\n",
    "\n",
    "# Option 1: Cosine Similarity on TF-IDF vectors\n",
    "similarity_matrix_tfidf = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# 4. Deduplication using Clustering (Unsupervised)\n",
    "# Using Agglomerative Clustering based on TF-IDF similarity\n",
    "n_clusters = 3 # Adjust based on expected number of unique entities\n",
    "distance_matrix_tfidf = 1 - similarity_matrix_tfidf\n",
    "clustering_tfidf = AgglomerativeClustering(n_clusters=n_clusters, linkage='average')\n",
    "cluster_labels_tfidf = clustering_tfidf.fit_predict(distance_matrix_tfidf)\n",
    "\n",
    "data['cluster_tfidf'] = cluster_labels_tfidf\n",
    "\n",
    "print(\"Deduplication using Agglomerative Clustering (TF-IDF):\")\n",
    "print(data[['name', 'email', 'address', 'cluster_tfidf']])\n",
    "\n",
    "# 5. Deduplication using Classification (Supervised - requires labeled data)\n",
    "# Let's create some synthetic labels for demonstration\n",
    "# In a real scenario, you would have labeled pairs of duplicates/non-duplicates\n",
    "pairs = []\n",
    "labels = []\n",
    "n_records = len(data)\n",
    "for i in range(n_records):\n",
    "    for j in range(i + 1, n_records):\n",
    "        pairs.append((i, j))\n",
    "        # Simple rule-based labeling for demonstration\n",
    "        if data['name'][i].split()[0] == data['name'][j].split()[0] and \\\n",
    "           data['address'][i].split()[0] == data['address'][j].split()[0]:\n",
    "            labels.append(1) # Duplicate\n",
    "        else:\n",
    "            labels.append(0) # Not a duplicate\n",
    "\n",
    "pairs_df = pd.DataFrame(pairs, columns=['record_index_1', 'record_index_2'])\n",
    "labels_df = pd.DataFrame(labels, columns=['is_duplicate'])\n",
    "pair_features = []\n",
    "\n",
    "for index1, index2 in pairs:\n",
    "    # Create feature vector for each pair (you can get creative here)\n",
    "    combined_feature_vector = np.concatenate((tfidf_matrix[index1].toarray().flatten(), tfidf_matrix[index2].toarray().flatten()))\n",
    "    pair_features.append(combined_feature_vector)\n",
    "\n",
    "X = np.array(pair_features)\n",
    "y = labels_df['is_duplicate'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"\\nClassification Report for Duplicate Detection:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Identifying duplicate groups based on the classifier\n",
    "duplicate_groups = {}\n",
    "for i, (index1, index2) in enumerate(pairs):\n",
    "    if model.predict(X[i].reshape(1, -1))[0] == 1:\n",
    "        if index1 not in duplicate_groups:\n",
    "            duplicate_groups[index1] = [index1]\n",
    "        if index2 not in duplicate_groups:\n",
    "            duplicate_groups[index2] = [index2]\n",
    "        if index2 not in duplicate_groups[index1]:\n",
    "            duplicate_groups[index1].append(index2)\n",
    "        if index1 not in duplicate_groups[index2]:\n",
    "            duplicate_groups[index2].append(index1)\n",
    "\n",
    "final_duplicate_sets = []\n",
    "seen = set()\n",
    "for group_indices in duplicate_groups.values():\n",
    "    group_tuple = tuple(sorted(group_indices))\n",
    "    if group_tuple not in seen:\n",
    "        final_duplicate_sets.append(list(group_tuple))\n",
    "        seen.add(group_tuple)\n",
    "\n",
    "print(\"\\nIdentified Duplicate Groups (based on classification):\")\n",
    "for group in final_duplicate_sets:\n",
    "    print([data['name'][i] for i in group])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
