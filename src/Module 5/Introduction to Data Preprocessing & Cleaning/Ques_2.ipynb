{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values:\n",
      "col1    1\n",
      "col2    1\n",
      "col3    1\n",
      "dtype: int64\n",
      "------------------------------\n",
      "DataFrame with missing values filled:\n",
      "   col1 col2  col3\n",
      "0   1.0    A  10.0\n",
      "1   2.0    A  20.0\n",
      "2   3.0    B  30.0\n",
      "3   4.0    A  27.5\n",
      "4   5.0    C  50.0\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# --- Task 1: Load a dataset and identify columns with missing values ---\n",
    "def identify_missing_values(df):\n",
    "    \"\"\"\n",
    "    Loads a dataset and identifies columns with missing values.\n",
    "\n",
    "    Args:\n",
    "        df: pandas DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        A pandas Series indicating the count of missing values per column.\n",
    "    \"\"\"\n",
    "    missing_counts = df.isnull().sum()\n",
    "    missing_cols = missing_counts[missing_counts > 0]\n",
    "    print(\"Columns with missing values:\")\n",
    "    print(missing_cols)\n",
    "    return missing_cols\n",
    "\n",
    "# Example for Task 1:\n",
    "data_missing = {'col1': [1, 2, np.nan, 4, 5],\n",
    "                'col2': ['A', np.nan, 'B', 'A', 'C'],\n",
    "                'col3': [10.0, 20.0, 30.0, np.nan, 50.0]}\n",
    "df_missing = pd.DataFrame(data_missing)\n",
    "identify_missing_values(df_missing)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- Task 2: Replace missing values with mean or mode ---\n",
    "def fill_missing_values(df):\n",
    "    \"\"\"\n",
    "    Replaces missing values in a dataset with the column mean for numeric columns\n",
    "    and the column mode for categorical columns.\n",
    "\n",
    "    Args:\n",
    "        df: pandas DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame with missing values filled.\n",
    "    \"\"\"\n",
    "    df_filled = df.copy()\n",
    "    for col in df_filled.columns:\n",
    "        if df_filled[col].isnull().any():\n",
    "            if pd.api.types.is_numeric_dtype(df_filled[col]):\n",
    "                mean_val = df_filled[col].mean()\n",
    "                df_filled[col].fillna(mean_val, inplace=True)\n",
    "            else:\n",
    "                mode_val = df_filled[col].mode()[0]\n",
    "                df_filled[col].fillna(mode_val, inplace=True)\n",
    "    print(\"DataFrame with missing values filled:\")\n",
    "    print(df_filled)\n",
    "    return df_filled\n",
    "\n",
    "# Example for Task 2:\n",
    "df_filled = fill_missing_values(df_missing.copy())\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- Task 3: Compare model performance with and without handling missing values ---\n",
    "def compare_model_performance(df_original, df_filled, target_column):\n",
    "    \"\"\"\n",
    "    Compares the performance of a linear regression model trained on a dataset\n",
    "    with and without handling missing values.\n",
    "\n",
    "    Args:\n",
    "        df_original: pandas DataFrame with missing values.\n",
    "        df_filled: pandas DataFrame with missing values filled.\n",
    "        target_column: Name of the target variable column.\n",
    "\n",
    "    Returns:\n",
    "        None. Prints the Mean Squared Error for both scenarios.\n",
    "    \"\"\"\n",
    "    # Separate features and target\n",
    "    if target_column not in df_original.columns:\n",
    "        print(f\"Error: Target column '{target_column}' not found.\")\n",
    "        return\n",
    "\n",
    "    X_original = df_original.drop(target_column, axis=1)\n",
    "    y_original = df_original[target_column]\n",
    "\n",
    "    X_filled = df_filled.drop(target_column, axis=1)\n",
    "    y_filled = df_filled[target_column]\n",
    "\n",
    "    # Handle categorical features (simple label encoding for this example)\n",
    "    for col in X_original.columns:\n",
    "        if not pd.api.types.is_numeric_dtype(X_original[col]):\n",
    "            le = LabelEncoder()\n",
    "            X_original[col] = le.fit_transform(X_original[col])\n",
    "            X_filled[col] = le.transform(X_filled[col]) # Use the same encoder\n",
    "\n",
    "    # Split data\n",
    "    X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(X_original, y_original, test_size=0.3, random_state=42)\n",
    "    X_train_filled, X_test_filled, y_train_filled, y_test_filled = train_test_split(X_filled, y_filled, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Train and evaluate model on original data (with missing values)\n",
    "    model_original = LinearRegression()\n",
    "    try:\n",
    "        model_original.fit(X_train_orig, y_train_orig)\n",
    "        predictions_original = model_original.predict(X_test_orig)\n",
    "        mse_original = mean_squared_error(y_test_orig, predictions_original)\n",
    "        print(f\"\\nModel Performance (Original Data - Missing Values):\")\n",
    "        print(f\"Mean Squared Error: {mse_original:.2f}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"\\nError training model on original data: {e}\")\n",
    "        print(\"Consider handling missing values before training.\")\n",
    "\n",
    "    # Train and evaluate model on filled data\n",
    "    model_filled = LinearRegression()\n",
    "    model_filled.fit(X_train_filled, y_train_filled)\n",
    "    predictions_filled = model_filled.predict(X_test_filled)\n",
    "    mse_filled = mean_squared_error(y_test_filled, predictions_filled)\n",
    "    print(f\"\\nModel Performance (Data with Missing Values Filled):\")\n",
    "    print(f\"Mean Squared Error: {mse_filled:.2f}\")\n",
    "\n",
    "# Example for Task 3:\n",
    "data_for_model = {'feature1': [1, 2, np.nan, 4, 5, 6, 7, np.nan, 9, 10],\n",
    "                  'feature2': ['A', 'B', 'A', 'C', 'B', 'A', 'B', 'A', 'C', 'B'],\n",
    "                  'target': [10, 20, 30, np.nan, 50, 60, 70, 80, 90, 100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifying duplicate rows...\n",
      "Number of duplicate rows found: 3\n",
      "\n",
      "Removing duplicate rows...\n",
      "Duplicate rows removed.\n",
      "------------------------------\n",
      "Dataset shape before removing duplicates: (8, 3)\n",
      "Dataset shape after removing duplicates: (5, 3)\n",
      "------------------------------\n",
      "\n",
      "    Hey classmate, let's talk about how duplicate data can mess up our predictions in machine learning.\n",
      "\n",
      "    Imagine you're training a model to predict whether a customer will buy a product. If you have many identical entries for the same customer (same features, same outcome), the model might:\n",
      "\n",
      "    1. **Overemphasize certain data points:** The model will see these repeated data points as more important than they actually are in the overall population. It will essentially give these duplicates more \"weight\" during training.\n",
      "\n",
      "    2. **Lead to biased learning:** If the duplicates happen to be clustered around a specific outcome, the model might learn a skewed relationship that isn't representative of the true patterns in the data. For example, if you have many duplicate entries of customers who didn't buy the product, the model might become overly confident in predicting that no one will buy it.\n",
      "\n",
      "    3. **Inflate performance metrics on the training data (but perform poorly on unseen data):** The model might achieve high accuracy on the training set simply because it has seen the same data points multiple times. However, when it encounters new, unseen data, its performance could be significantly worse because it hasn't learned a generalizable pattern. This is a form of overfitting.\n",
      "\n",
      "    4. **Waste computational resources:** Training on a dataset with many duplicates can take longer and consume more memory without providing any additional valuable information to the model.\n",
      "\n",
      "    In short, duplicate data can prevent our models from learning the true underlying relationships in the data and can lead to inaccurate predictions on new data. That's why it's crucial to identify and remove duplicates during the data cleaning process!\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Task 1: Identify and remove duplicate entries ---\n",
    "def identify_and_remove_duplicates(df):\n",
    "    \"\"\"\n",
    "    Identifies and removes duplicate entries from a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df: pandas DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame with duplicate rows removed.\n",
    "    \"\"\"\n",
    "    print(\"Identifying duplicate rows...\")\n",
    "    duplicate_rows = df[df.duplicated()]\n",
    "    print(\"Number of duplicate rows found:\", len(duplicate_rows))\n",
    "\n",
    "    print(\"\\nRemoving duplicate rows...\")\n",
    "    df_no_duplicates = df.drop_duplicates()\n",
    "    print(\"Duplicate rows removed.\")\n",
    "    return df_no_duplicates\n",
    "\n",
    "# Example for Task 1:\n",
    "data_duplicates = {'col1': [1, 2, 2, 3, 4, 4, 4, 5],\n",
    "                   'col2': ['A', 'B', 'B', 'C', 'D', 'D', 'D', 'E'],\n",
    "                   'col3': [10, 20, 20, 30, 40, 40, 40, 50]}\n",
    "df_with_duplicates = pd.DataFrame(data_duplicates)\n",
    "df_without_duplicates = identify_and_remove_duplicates(df_with_duplicates.copy())\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- Task 2: Document the before-and-after dataset shape ---\n",
    "def document_dataset_shape(df_before, df_after):\n",
    "    \"\"\"\n",
    "    Documents the shape of a pandas DataFrame before and after removing duplicates.\n",
    "\n",
    "    Args:\n",
    "        df_before: pandas DataFrame before duplicate removal.\n",
    "        df_after: pandas DataFrame after duplicate removal.\n",
    "    \"\"\"\n",
    "    print(\"Dataset shape before removing duplicates:\", df_before.shape)\n",
    "    print(\"Dataset shape after removing duplicates:\", df_after.shape)\n",
    "\n",
    "# Example for Task 2:\n",
    "document_dataset_shape(df_with_duplicates, df_without_duplicates)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- Task 3: Explain to a classmate how duplicate data can affect prediction accuracy ---\n",
    "def explain_duplicate_data_impact():\n",
    "    \"\"\"\n",
    "    Provides an explanation of how duplicate data can affect prediction accuracy.\n",
    "    \"\"\"\n",
    "    explanation = \"\"\"\n",
    "    Hey classmate, let's talk about how duplicate data can mess up our predictions in machine learning.\n",
    "\n",
    "    Imagine you're training a model to predict whether a customer will buy a product. If you have many identical entries for the same customer (same features, same outcome), the model might:\n",
    "\n",
    "    1. **Overemphasize certain data points:** The model will see these repeated data points as more important than they actually are in the overall population. It will essentially give these duplicates more \"weight\" during training.\n",
    "\n",
    "    2. **Lead to biased learning:** If the duplicates happen to be clustered around a specific outcome, the model might learn a skewed relationship that isn't representative of the true patterns in the data. For example, if you have many duplicate entries of customers who didn't buy the product, the model might become overly confident in predicting that no one will buy it.\n",
    "\n",
    "    3. **Inflate performance metrics on the training data (but perform poorly on unseen data):** The model might achieve high accuracy on the training set simply because it has seen the same data points multiple times. However, when it encounters new, unseen data, its performance could be significantly worse because it hasn't learned a generalizable pattern. This is a form of overfitting.\n",
    "\n",
    "    4. **Waste computational resources:** Training on a dataset with many duplicates can take longer and consume more memory without providing any additional valuable information to the model.\n",
    "\n",
    "    In short, duplicate data can prevent our models from learning the true underlying relationships in the data and can lead to inaccurate predictions on new data. That's why it's crucial to identify and remove duplicates during the data cleaning process!\n",
    "    \"\"\"\n",
    "    print(explanation)\n",
    "\n",
    "# Example for Task 3:\n",
    "explain_duplicate_data_impact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with string numbers:\n",
      "id             int64\n",
      "values_str    object\n",
      "other_col     object\n",
      "dtype: object\n",
      "Column 'values_str' successfully converted to integer.\n",
      "\n",
      "DataFrame after conversion:\n",
      "id             int64\n",
      "values_str      int8\n",
      "other_col     object\n",
      "dtype: object\n",
      "------------------------------\n",
      "DataFrame with inconsistent types:\n",
      "id         object\n",
      "amounts    object\n",
      "flags      object\n",
      "dtype: object\n",
      "Identifying and correcting inconsistent data types...\n",
      "Column 'id' changed from object to int64\n",
      "Column 'amounts' changed from object to float64\n",
      "Potential inconsistent data types addressed.\n",
      "\n",
      "DataFrame after potential correction:\n",
      "id           int64\n",
      "amounts    float64\n",
      "flags       object\n",
      "dtype: object\n",
      "------------------------------\n",
      "\n",
      "    Correct data types are absolutely critical for effective feature engineering for several reasons:\n",
      "\n",
      "    1. **Enabling Mathematical Operations:** Many feature engineering techniques involve mathematical operations like arithmetic, aggregation (sum, mean, etc.), and comparisons. These operations can only be reliably performed on numeric data types (integers, floats). If a column containing numerical information is stored as a string, these operations will either fail or produce incorrect results. Converting to the correct numeric type is essential.\n",
      "\n",
      "    2. **Facilitating Categorical Encoding:** Feature engineering often involves encoding categorical variables into a numerical format that machine learning models can understand (e.g., one-hot encoding, label encoding). To apply these encoding techniques correctly, the categorical columns need to be identified with the appropriate data type (e.g., 'object' or 'category' in pandas). Inconsistent or incorrect typing of categorical columns can lead to errors or ineffective encoding.\n",
      "\n",
      "    3. **Supporting Date and Time Manipulation:** When dealing with time-based data, having the correct datetime data type is crucial for performing operations like calculating time differences, extracting specific time components (year, month, day, hour), and creating time-based features (e.g., lag features, rolling statistics). Incorrectly typed date or time columns (e.g., as strings or integers) will make these powerful feature engineering techniques difficult or impossible to implement correctly.\n",
      "\n",
      "    4. **Ensuring Compatibility with Libraries and Models:** Machine learning libraries like scikit-learn often have specific data type requirements for their functions and models. Providing data with incorrect types can lead to errors during training or prediction. Ensuring correct types makes your data compatible with these tools.\n",
      "\n",
      "    5. **Improving Efficiency and Reducing Memory Usage:** Correct data types can also impact the efficiency of your code and the memory usage of your data. For example, storing integers as 'float64' consumes more memory than storing them as 'int64'. Choosing the most appropriate data type can lead to more efficient feature engineering processes.\n",
      "\n",
      "    In summary, correct data types form the foundation for meaningful and effective feature engineering. They ensure that the intended operations can be performed accurately, that categorical and time-based features can be manipulated correctly, and that the data is compatible with machine learning algorithms and libraries. Investing time in identifying and correcting data types is a crucial step in the data preprocessing pipeline.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Task 1: Convert a column of string numbers to integers ---\n",
    "def convert_string_to_int(df, column_name):\n",
    "    \"\"\"\n",
    "    Converts a column of string numbers to integers in a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df: pandas DataFrame.\n",
    "        column_name: Name of the column to convert.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame with the specified column converted to integer type.\n",
    "        Returns the original DataFrame if the column is not found or already numeric.\n",
    "    \"\"\"\n",
    "    if column_name not in df.columns:\n",
    "        print(f\"Error: Column '{column_name}' not found.\")\n",
    "        return df\n",
    "\n",
    "    if pd.api.types.is_numeric_dtype(df[column_name]):\n",
    "        print(f\"Column '{column_name}' is already numeric.\")\n",
    "        return df\n",
    "\n",
    "    try:\n",
    "        df[column_name] = pd.to_numeric(df[column_name], errors='raise', downcast='integer')\n",
    "        print(f\"Column '{column_name}' successfully converted to integer.\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error converting column '{column_name}' to integer: {e}\")\n",
    "        print(\"Ensure the column contains valid numeric strings.\")\n",
    "    return df\n",
    "\n",
    "# Example for Task 1:\n",
    "data_string_numbers = {'id': [1, 2, 3],\n",
    "                       'values_str': ['10', '25', '100'],\n",
    "                       'other_col': ['A', 'B', 'C']}\n",
    "df_string = pd.DataFrame(data_string_numbers)\n",
    "print(\"DataFrame with string numbers:\")\n",
    "print(df_string.dtypes)\n",
    "df_int = convert_string_to_int(df_string.copy(), 'values_str')\n",
    "print(\"\\nDataFrame after conversion:\")\n",
    "print(df_int.dtypes)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- Task 2: Identify and correct columns with inconsistent data types ---\n",
    "def identify_and_correct_inconsistent_types(df):\n",
    "    \"\"\"\n",
    "    Identifies and attempts to correct columns with inconsistent data types\n",
    "    in a pandas DataFrame. This function will try to infer the best data type.\n",
    "\n",
    "    Args:\n",
    "        df: pandas DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame with potentially corrected data types.\n",
    "    \"\"\"\n",
    "    print(\"Identifying and correcting inconsistent data types...\")\n",
    "    df_corrected = df.copy()\n",
    "    for col in df_corrected.columns:\n",
    "        original_dtype = df_corrected[col].dtype\n",
    "        try:\n",
    "            df_corrected[col] = pd.to_numeric(df_corrected[col], errors='ignore')\n",
    "            if df_corrected[col].dtype != original_dtype:\n",
    "                print(f\"Column '{col}' changed from {original_dtype} to {df_corrected[col].dtype}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not automatically correct type for column '{col}': {e}\")\n",
    "    print(\"Potential inconsistent data types addressed.\")\n",
    "    return df_corrected\n",
    "\n",
    "# Example for Task 2:\n",
    "data_inconsistent = {'id': [1, '2', 3],\n",
    "                      'amounts': ['10.5', '20', 30.0],\n",
    "                      'flags': [True, 'False', 1]}\n",
    "df_inconsistent = pd.DataFrame(data_inconsistent)\n",
    "print(\"DataFrame with inconsistent types:\")\n",
    "print(df_inconsistent.dtypes)\n",
    "df_consistent = identify_and_correct_inconsistent_types(df_inconsistent.copy())\n",
    "print(\"\\nDataFrame after potential correction:\")\n",
    "print(df_consistent.dtypes)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- Task 3: Discuss why correct data types are critical for feature engineering ---\n",
    "def discuss_data_types_for_feature_engineering():\n",
    "    \"\"\"\n",
    "    Provides a discussion on why correct data types are critical for feature engineering.\n",
    "    \"\"\"\n",
    "    discussion = \"\"\"\n",
    "    Correct data types are absolutely critical for effective feature engineering for several reasons:\n",
    "\n",
    "    1. **Enabling Mathematical Operations:** Many feature engineering techniques involve mathematical operations like arithmetic, aggregation (sum, mean, etc.), and comparisons. These operations can only be reliably performed on numeric data types (integers, floats). If a column containing numerical information is stored as a string, these operations will either fail or produce incorrect results. Converting to the correct numeric type is essential.\n",
    "\n",
    "    2. **Facilitating Categorical Encoding:** Feature engineering often involves encoding categorical variables into a numerical format that machine learning models can understand (e.g., one-hot encoding, label encoding). To apply these encoding techniques correctly, the categorical columns need to be identified with the appropriate data type (e.g., 'object' or 'category' in pandas). Inconsistent or incorrect typing of categorical columns can lead to errors or ineffective encoding.\n",
    "\n",
    "    3. **Supporting Date and Time Manipulation:** When dealing with time-based data, having the correct datetime data type is crucial for performing operations like calculating time differences, extracting specific time components (year, month, day, hour), and creating time-based features (e.g., lag features, rolling statistics). Incorrectly typed date or time columns (e.g., as strings or integers) will make these powerful feature engineering techniques difficult or impossible to implement correctly.\n",
    "\n",
    "    4. **Ensuring Compatibility with Libraries and Models:** Machine learning libraries like scikit-learn often have specific data type requirements for their functions and models. Providing data with incorrect types can lead to errors during training or prediction. Ensuring correct types makes your data compatible with these tools.\n",
    "\n",
    "    5. **Improving Efficiency and Reducing Memory Usage:** Correct data types can also impact the efficiency of your code and the memory usage of your data. For example, storing integers as 'float64' consumes more memory than storing them as 'int64'. Choosing the most appropriate data type can lead to more efficient feature engineering processes.\n",
    "\n",
    "    In summary, correct data types form the foundation for meaningful and effective feature engineering. They ensure that the intended operations can be performed accurately, that categorical and time-based features can be manipulated correctly, and that the data is compatible with machine learning algorithms and libraries. Investing time in identifying and correcting data types is a crucial step in the data preprocessing pipeline.\n",
    "    \"\"\"\n",
    "    print(discussion)\n",
    "\n",
    "# Example for Task 3:\n",
    "discuss_data_types_for_feature_engineering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 98) (572954103.py, line 98)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 98\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(f\"Outliers (Z-score > {z\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 98)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# --- Task 1: Visualize and identify outliers using a boxplot ---\n",
    "def visualize_outliers_boxplot(df, columns):\n",
    "    \"\"\"\n",
    "    Visualizes the distribution of specified columns using boxplots to identify outliers.\n",
    "\n",
    "    Args:\n",
    "        df: pandas DataFrame.\n",
    "        columns: A list of column names to visualize.\n",
    "    \"\"\"\n",
    "    print(\"Visualizing outliers using boxplots...\")\n",
    "    plt.figure(figsize=(15, 6 * len(columns)))\n",
    "    for i, column in enumerate(columns):\n",
    "        if column in df.columns and pd.api.types.is_numeric_dtype(df[column]):\n",
    "            plt.subplot(len(columns), 1, i + 1)\n",
    "            sns.boxplot(x=df[column])\n",
    "            plt.title(f'Boxplot of {column}')\n",
    "        else:\n",
    "            print(f\"Warning: Column '{column}' is not numeric or not found.\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example for Task 1:\n",
    "data_outliers = {'col1': [10, 12, 15, 18, 20, 100],\n",
    "                  'col2': [5, 7, 9, 11, 13, -5],\n",
    "                  'col3': ['A', 'B', 'A', 'C', 'B', 'A']}\n",
    "df_outliers = pd.DataFrame(data_outliers)\n",
    "visualize_outliers_boxplot(df_outliers, ['col1', 'col2'])\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- Task 2: Remove or adjust outliers and re-analyze ---\n",
    "def handle_outliers_adjust_reanalyze(df, columns, method='remove', iqr_factor=1.5, zscore_threshold=3):\n",
    "    \"\"\"\n",
    "    Removes or adjusts outliers in specified numeric columns and re-analyzes the dataset\n",
    "    by printing descriptive statistics and visualizing boxplots.\n",
    "\n",
    "    Args:\n",
    "        df: pandas DataFrame.\n",
    "        columns: A list of numeric column names to handle outliers in.\n",
    "        method: 'remove' to remove outliers, 'cap' to cap them at the bounds,\n",
    "                'median' to replace with the median. Default is 'remove'.\n",
    "        iqr_factor: The factor to use for IQR-based outlier detection (if applicable). Default is 1.5.\n",
    "        zscore_threshold: The Z-score threshold for outlier detection (if applicable). Default is 3.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame with handled outliers.\n",
    "    \"\"\"\n",
    "    df_handled = df.copy()\n",
    "    print(f\"\\nHandling outliers using '{method}' method...\")\n",
    "\n",
    "    for column in columns:\n",
    "        if column in df_handled.columns and pd.api.types.is_numeric_dtype(df_handled[column]):\n",
    "            print(f\"\\nHandling outliers in column '{column}':\")\n",
    "            if method in ['remove', 'cap', 'median']:\n",
    "                if method == 'remove':\n",
    "                    # IQR based removal\n",
    "                    Q1 = df_handled[column].quantile(0.25)\n",
    "                    Q3 = df_handled[column].quantile(0.75)\n",
    "                    IQR = Q3 - Q1\n",
    "                    lower_bound = Q1 - iqr_factor * IQR\n",
    "                    upper_bound = Q3 + iqr_factor * IQR\n",
    "                    df_handled = df_handled[(df_handled[column] >= lower_bound) & (df_handled[column] <= upper_bound)]\n",
    "                    print(\"Outliers removed based on IQR.\")\n",
    "                elif method == 'cap':\n",
    "                    # IQR based capping\n",
    "                    Q1 = df_handled[column].quantile(0.25)\n",
    "                    Q3 = df_handled[column].quantile(0.75)\n",
    "                    IQR = Q3 - Q1\n",
    "                    lower_bound = Q1 - iqr_factor * IQR\n",
    "                    upper_bound = Q3 + iqr_factor * IQR\n",
    "                    df_handled[column] = np.where(df_handled[column] < lower_bound, lower_bound, df_handled[column])\n",
    "                    df_handled[column] = np.where(df_handled[column] > upper_bound, upper_bound, df_handled[column])\n",
    "                    print(\"Outliers capped based on IQR.\")\n",
    "                elif method == 'median':\n",
    "                    # IQR based replacement with median\n",
    "                    Q1 = df_handled[column].quantile(0.25)\n",
    "                    Q3 = df_handled[column].quantile(0.75)\n",
    "                    IQR = Q3 - Q1\n",
    "                    lower_bound = Q1 - iqr_factor * IQR\n",
    "                    upper_bound = Q3 + iqr_factor * IQR\n",
    "                    median_val = df_handled[column].median()\n",
    "                    df_handled[column] = np.where((df_handled[column] < lower_bound) | (df_handled[column] > upper_bound), median_val, df_handled[column])\n",
    "                    print(\"Outliers replaced with median based on IQR.\")\n",
    "            elif method == 'zscore':\n",
    "                # Z-score based handling (replace with median)\n",
    "                mean_val = df_handled[column].mean()\n",
    "                std_dev = df_handled[column].std()\n",
    "                if std_dev != 0:\n",
    "                    z_scores = np.abs(stats.zscore(df_handled[column]))\n",
    "                    outlier_indices = z_scores > zscore_threshold\n",
    "                    median_val = df_handled[column].median()\n",
    "                    df_handled.loc[outlier_indices, column] = median_val\n",
    "                    print(f\"Outliers (Z-score > {z"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
